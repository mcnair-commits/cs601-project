{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e092cd7-1711-4e81-8f76-d65719cfe3c4",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbff71d-1ed4-41e8-842e-430cd81f1c20",
   "metadata": {},
   "source": [
    "## Kelton McNair - 992018683\n",
    "### CS 601 - Machine Learning\n",
    "### Spring Semester Project 2025\n",
    "### Dr. Afshar\n",
    "### Random Forest Classifier\n",
    "### Dataset: Adult "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9d4bab-68b9-42f7-b2b1-040f36af3198",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# # fetch dataset \n",
    "# letter_recognition = fetch_ucirepo(id=59) \n",
    "  \n",
    "# # data (as pandas dataframes) \n",
    "# X = letter_recognition.data.features \n",
    "# y = letter_recognition.data.targets \n",
    "  \n",
    "# # metadata \n",
    "# print(letter_recognition.metadata) \n",
    "  \n",
    "# # variable information \n",
    "# print(letter_recognition.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a9f424-5098-4214-beb8-afe37b923c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ucimlrepo\n",
    "\n",
    "# from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# # fetch dataset \n",
    "# adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# # data (as pandas dataframes) \n",
    "# X = adult.data.features \n",
    "# y = adult.data.targets \n",
    "  \n",
    "# # metadata \n",
    "# print(adult.metadata) \n",
    "  \n",
    "# # variable information \n",
    "# print(adult.variables) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18976bf9-2f7e-4092-9cb1-1cc1de604d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import pandas as pd\n",
    "from mcnair_rf_classifier import imported_random_forest_classifier\n",
    "\n",
    "from multiprocessing import Pool # multiprocessing but requires function to be in a .py file when working with jupyter?\n",
    "from multiprocessing import get_context #older fork method not newer spawn, issues with this also, need to put functions in .py file maybe\n",
    "\n",
    "#not sure what all I need for comparative evaluation sklearn model yet\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "rng = default_rng(110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1a78a4-4025-4938-8d47-95ef80c82113",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"adult/adult.data\") #na_values didnt work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fefd73ed-a649-44ad-a4d4-e6a72d9505a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_col_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0721cf4-fcf1-4c24-a26d-626da02e65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = extracted_col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72848efb-24d7-4c0d-91f3-27caa31ea87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16934</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>69739</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19497</th>\n",
       "      <td>58</td>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>129786</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22836</th>\n",
       "      <td>24</td>\n",
       "      <td>Private</td>\n",
       "      <td>67804</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age     workclass  fnlwgt education  education-num  \\\n",
       "16934   25       Private   69739      10th              6   \n",
       "19497   58   Federal-gov  129786      10th              6   \n",
       "22836   24       Private   67804   HS-grad              9   \n",
       "\n",
       "            marital-status          occupation relationship    race    sex  \\\n",
       "16934        Never-married   Machine-op-inspct    Own-child   White   Male   \n",
       "19497   Married-civ-spouse        Craft-repair      Husband   White   Male   \n",
       "22836        Never-married       Other-service    Own-child   Black   Male   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week  native-country  income  \n",
       "16934             0             0              40   United-States   <=50K  \n",
       "19497             0             0              40   United-States   <=50K  \n",
       "22836             0             0              40   United-States   <=50K  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0854f978-62cb-43c3-95af-675943256355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32555</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32560 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  workclass  fnlwgt  education  education-num  marital-status  \\\n",
       "0      False      False   False      False          False           False   \n",
       "1      False      False   False      False          False           False   \n",
       "2      False      False   False      False          False           False   \n",
       "3      False      False   False      False          False           False   \n",
       "4      False      False   False      False          False           False   \n",
       "...      ...        ...     ...        ...            ...             ...   \n",
       "32555  False      False   False      False          False           False   \n",
       "32556  False      False   False      False          False           False   \n",
       "32557  False      False   False      False          False           False   \n",
       "32558  False      False   False      False          False           False   \n",
       "32559  False      False   False      False          False           False   \n",
       "\n",
       "       occupation  relationship   race    sex  capital-gain  capital-loss  \\\n",
       "0           False         False  False  False         False         False   \n",
       "1           False         False  False  False         False         False   \n",
       "2           False         False  False  False         False         False   \n",
       "3           False         False  False  False         False         False   \n",
       "4           False         False  False  False         False         False   \n",
       "...           ...           ...    ...    ...           ...           ...   \n",
       "32555       False         False  False  False         False         False   \n",
       "32556       False         False  False  False         False         False   \n",
       "32557       False         False  False  False         False         False   \n",
       "32558       False         False  False  False         False         False   \n",
       "32559       False         False  False  False         False         False   \n",
       "\n",
       "       hours-per-week  native-country  income  \n",
       "0               False           False   False  \n",
       "1               False           False   False  \n",
       "2               False           False   False  \n",
       "3               False           False   False  \n",
       "4               False           False   False  \n",
       "...               ...             ...     ...  \n",
       "32555           False           False   False  \n",
       "32556           False           False   False  \n",
       "32557           False           False   False  \n",
       "32558           False           False   False  \n",
       "32559           False           False   False  \n",
       "\n",
       "[32560 rows x 15 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e6260ea-6f97-4b0f-a472-eeb089d6ff76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education-num     0\n",
       "marital-status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital-gain      0\n",
       "capital-loss      0\n",
       "hours-per-week    0\n",
       "native-country    0\n",
       "income            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "006675ea-0a79-429f-8a38-1c1227370a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               False\n",
       "workclass         False\n",
       "fnlwgt            False\n",
       "education         False\n",
       "education-num     False\n",
       "marital-status    False\n",
       "occupation        False\n",
       "relationship      False\n",
       "race              False\n",
       "sex               False\n",
       "capital-gain      False\n",
       "capital-loss      False\n",
       "hours-per-week    False\n",
       "native-country    False\n",
       "income            False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are not seeing any NA values or missing values, but if we check value counts below, we see that there are values with a ? as the value\n",
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cd76d8c-2215-43ed-89a6-e6e3b2de0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in data.columns:\n",
    "#     print(data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c0546f0-bc56-49f1-9963-22e2345068da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "36    1348\n",
      "35    1337\n",
      "33    1335\n",
      "23    1329\n",
      "31    1325\n",
      "      ... \n",
      "88       6\n",
      "85       5\n",
      "87       3\n",
      "89       2\n",
      "86       1\n",
      "Name: count, Length: 74, dtype: int64\n",
      "workclass\n",
      "Private             33906\n",
      "Self-emp-not-inc     3862\n",
      "Local-gov            3136\n",
      "?                    2799\n",
      "State-gov            1980\n",
      "Self-emp-inc         1695\n",
      "Federal-gov          1432\n",
      "Without-pay            21\n",
      "Never-worked           10\n",
      "Name: count, dtype: int64\n",
      "fnlwgt\n",
      "203488    21\n",
      "190290    19\n",
      "120277    19\n",
      "125892    18\n",
      "126569    18\n",
      "          ..\n",
      "158737     1\n",
      "286983     1\n",
      "185942     1\n",
      "234220     1\n",
      "350977     1\n",
      "Name: count, Length: 28522, dtype: int64\n",
      "education\n",
      "HS-grad         15784\n",
      "Some-college    10878\n",
      "Bachelors        8024\n",
      "Masters          2657\n",
      "Assoc-voc        2061\n",
      "11th             1812\n",
      "Assoc-acdm       1601\n",
      "10th             1389\n",
      "7th-8th           955\n",
      "Prof-school       834\n",
      "9th               756\n",
      "12th              657\n",
      "Doctorate         594\n",
      "5th-6th           509\n",
      "1st-4th           247\n",
      "Preschool          83\n",
      "Name: count, dtype: int64\n",
      "education-num\n",
      "9     15784\n",
      "10    10878\n",
      "13     8024\n",
      "14     2657\n",
      "11     2061\n",
      "7      1812\n",
      "12     1601\n",
      "6      1389\n",
      "4       955\n",
      "15      834\n",
      "5       756\n",
      "8       657\n",
      "16      594\n",
      "3       509\n",
      "2       247\n",
      "1        83\n",
      "Name: count, dtype: int64\n",
      "marital-status\n",
      "Married-civ-spouse       22379\n",
      "Never-married            16116\n",
      "Divorced                  6633\n",
      "Separated                 1530\n",
      "Widowed                   1518\n",
      "Married-spouse-absent      628\n",
      "Married-AF-spouse           37\n",
      "Name: count, dtype: int64\n",
      "occupation\n",
      "Prof-specialty       6172\n",
      "Craft-repair         6112\n",
      "Exec-managerial      6086\n",
      "Adm-clerical         5610\n",
      "Sales                5504\n",
      "Other-service        4923\n",
      "Machine-op-inspct    3022\n",
      "?                    2809\n",
      "Transport-moving     2355\n",
      "Handlers-cleaners    2072\n",
      "Farming-fishing      1490\n",
      "Tech-support         1446\n",
      "Protective-serv       983\n",
      "Priv-house-serv       242\n",
      "Armed-Forces           15\n",
      "Name: count, dtype: int64\n",
      "relationship\n",
      "Husband           19716\n",
      "Not-in-family     12582\n",
      "Own-child          7581\n",
      "Unmarried          5125\n",
      "Wife               2331\n",
      "Other-relative     1506\n",
      "Name: count, dtype: int64\n",
      "race\n",
      "White                 41761\n",
      "Black                  4685\n",
      "Asian-Pac-Islander     1519\n",
      "Amer-Indian-Eskimo      470\n",
      "Other                   406\n",
      "Name: count, dtype: int64\n",
      "sex\n",
      "Male      32649\n",
      "Female    16192\n",
      "Name: count, dtype: int64\n",
      "capital-gain\n",
      "0        44807\n",
      "15024      513\n",
      "7688       410\n",
      "7298       364\n",
      "99999      244\n",
      "         ...  \n",
      "22040        1\n",
      "2387         1\n",
      "1639         1\n",
      "1111         1\n",
      "6612         1\n",
      "Name: count, Length: 123, dtype: int64\n",
      "capital-loss\n",
      "0       46559\n",
      "1902      304\n",
      "1977      253\n",
      "1887      233\n",
      "2415       72\n",
      "        ...  \n",
      "1539        1\n",
      "1870        1\n",
      "1911        1\n",
      "2465        1\n",
      "1421        1\n",
      "Name: count, Length: 99, dtype: int64\n",
      "hours-per-week\n",
      "40    22802\n",
      "50     4246\n",
      "45     2717\n",
      "60     2177\n",
      "35     1937\n",
      "      ...  \n",
      "94        1\n",
      "82        1\n",
      "87        1\n",
      "79        1\n",
      "69        1\n",
      "Name: count, Length: 96, dtype: int64\n",
      "native-country\n",
      "United-States                 43831\n",
      "Mexico                          951\n",
      "?                               857\n",
      "Philippines                     295\n",
      "Germany                         206\n",
      "Puerto-Rico                     184\n",
      "Canada                          182\n",
      "El-Salvador                     155\n",
      "India                           151\n",
      "Cuba                            138\n",
      "England                         127\n",
      "China                           122\n",
      "South                           115\n",
      "Jamaica                         106\n",
      "Italy                           105\n",
      "Dominican-Republic              103\n",
      "Japan                            92\n",
      "Guatemala                        88\n",
      "Poland                           87\n",
      "Vietnam                          86\n",
      "Columbia                         85\n",
      "Haiti                            75\n",
      "Portugal                         67\n",
      "Taiwan                           65\n",
      "Iran                             59\n",
      "Greece                           49\n",
      "Nicaragua                        49\n",
      "Peru                             46\n",
      "Ecuador                          45\n",
      "France                           38\n",
      "Ireland                          37\n",
      "Hong                             30\n",
      "Thailand                         30\n",
      "Cambodia                         28\n",
      "Trinadad&Tobago                  27\n",
      "Laos                             23\n",
      "Yugoslavia                       23\n",
      "Outlying-US(Guam-USVI-etc)       23\n",
      "Scotland                         21\n",
      "Honduras                         20\n",
      "Hungary                          19\n",
      "Holand-Netherlands                1\n",
      "Name: count, dtype: int64\n",
      "income\n",
      "<=50K     24719\n",
      "<=50K.    12435\n",
      ">50K       7841\n",
      ">50K.      3846\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for col in merged_train_test.columns:\n",
    "    print(merged_train_test[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6123a5-c111-4933-82b4-030c69928662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31c02ae4-740f-4a7b-82a9-989820ffcce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age - int64\n",
      "workclass - object\n",
      "fnlwgt - int64\n",
      "education - object\n",
      "education-num - int64\n",
      "marital-status - object\n",
      "occupation - object\n",
      "relationship - object\n",
      "race - object\n",
      "sex - object\n",
      "capital-gain - int64\n",
      "capital-loss - int64\n",
      "hours-per-week - int64\n",
      "native-country - object\n",
      "income - object\n"
     ]
    }
   ],
   "source": [
    "cols_categorical = [] # we use this in about 15 cells\n",
    "\n",
    "for col in data.columns:\n",
    "    current_data_type = data[col].dtype\n",
    "    print(col,'-',current_data_type)\n",
    "    if current_data_type != 'int64' and current_data_type != 'float64':\n",
    "        cols_categorical.append(col)\n",
    "        data[col] = data[col].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8533c50-7a47-4401-ba7c-3e3e2690d785",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                  0\n",
      "workclass         1836\n",
      "fnlwgt               0\n",
      "education            0\n",
      "education-num        0\n",
      "marital-status       0\n",
      "occupation        1843\n",
      "relationship         0\n",
      "race                 0\n",
      "sex                  0\n",
      "capital-gain         0\n",
      "capital-loss         0\n",
      "hours-per-week       0\n",
      "native-country     583\n",
      "income               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# for col in data.columns:\n",
    "#     x = data.apply(lambda col: (col == '?').sum())\n",
    "#     print (x)\n",
    "\n",
    "question_marks = data.apply(lambda col: (col == '?').sum())\n",
    "\n",
    "print (question_marks)\n",
    "\n",
    "# we see that there are anywhere between 1836 and 1836+1843+583 total samples with a ?\n",
    "# given that we are encoding with oneHot / dummy encoding, can you leave a ?, because it will be encoded into its own category bucket\n",
    "# or in this situation is it better to try and apply a \"nearby\" or similar value for these samples categories through something like knn/clustering/other method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00da1949-bd71-4831-9c31-557a7f1279e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['workclass',\n",
       " 'education',\n",
       " 'marital-status',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'native-country',\n",
       " 'income']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b8371d5-4272-4ab8-9e34-b9c3a5342967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "36    898\n",
      "31    888\n",
      "34    886\n",
      "23    877\n",
      "35    876\n",
      "     ... \n",
      "83      6\n",
      "88      3\n",
      "85      3\n",
      "86      1\n",
      "87      1\n",
      "Name: count, Length: 73, dtype: int64\n",
      "workclass\n",
      "Private             22696\n",
      "Self-emp-not-inc     2541\n",
      "Local-gov            2093\n",
      "?                    1836\n",
      "State-gov            1297\n",
      "Self-emp-inc         1116\n",
      "Federal-gov           960\n",
      "Without-pay            14\n",
      "Never-worked            7\n",
      "Name: count, dtype: int64\n",
      "fnlwgt\n",
      "164190    13\n",
      "203488    13\n",
      "123011    13\n",
      "148995    12\n",
      "126675    12\n",
      "          ..\n",
      "325573     1\n",
      "140176     1\n",
      "318264     1\n",
      "329205     1\n",
      "257302     1\n",
      "Name: count, Length: 21647, dtype: int64\n",
      "education\n",
      "HS-grad         10501\n",
      "Some-college     7291\n",
      "Bachelors        5354\n",
      "Masters          1723\n",
      "Assoc-voc        1382\n",
      "11th             1175\n",
      "Assoc-acdm       1067\n",
      "10th              933\n",
      "7th-8th           646\n",
      "Prof-school       576\n",
      "9th               514\n",
      "12th              433\n",
      "Doctorate         413\n",
      "5th-6th           333\n",
      "1st-4th           168\n",
      "Preschool          51\n",
      "Name: count, dtype: int64\n",
      "education-num\n",
      "9     10501\n",
      "10     7291\n",
      "13     5354\n",
      "14     1723\n",
      "11     1382\n",
      "7      1175\n",
      "12     1067\n",
      "6       933\n",
      "4       646\n",
      "15      576\n",
      "5       514\n",
      "8       433\n",
      "16      413\n",
      "3       333\n",
      "2       168\n",
      "1        51\n",
      "Name: count, dtype: int64\n",
      "marital-status\n",
      "Married-civ-spouse       14976\n",
      "Never-married            10682\n",
      "Divorced                  4443\n",
      "Separated                 1025\n",
      "Widowed                    993\n",
      "Married-spouse-absent      418\n",
      "Married-AF-spouse           23\n",
      "Name: count, dtype: int64\n",
      "occupation\n",
      "Prof-specialty       4140\n",
      "Craft-repair         4099\n",
      "Exec-managerial      4066\n",
      "Adm-clerical         3769\n",
      "Sales                3650\n",
      "Other-service        3295\n",
      "Machine-op-inspct    2002\n",
      "?                    1843\n",
      "Transport-moving     1597\n",
      "Handlers-cleaners    1370\n",
      "Farming-fishing       994\n",
      "Tech-support          928\n",
      "Protective-serv       649\n",
      "Priv-house-serv       149\n",
      "Armed-Forces            9\n",
      "Name: count, dtype: int64\n",
      "relationship\n",
      "Husband           13193\n",
      "Not-in-family      8304\n",
      "Own-child          5068\n",
      "Unmarried          3446\n",
      "Wife               1568\n",
      "Other-relative      981\n",
      "Name: count, dtype: int64\n",
      "race\n",
      "White                 27815\n",
      "Black                  3124\n",
      "Asian-Pac-Islander     1039\n",
      "Amer-Indian-Eskimo      311\n",
      "Other                   271\n",
      "Name: count, dtype: int64\n",
      "sex\n",
      "Male      21789\n",
      "Female    10771\n",
      "Name: count, dtype: int64\n",
      "capital-gain\n",
      "0        29849\n",
      "15024      347\n",
      "7688       284\n",
      "7298       246\n",
      "99999      159\n",
      "         ...  \n",
      "1111         1\n",
      "2538         1\n",
      "22040        1\n",
      "4931         1\n",
      "5060         1\n",
      "Name: count, Length: 119, dtype: int64\n",
      "capital-loss\n",
      "0       31041\n",
      "1902      202\n",
      "1977      168\n",
      "1887      159\n",
      "1848       51\n",
      "        ...  \n",
      "2080        1\n",
      "1539        1\n",
      "1844        1\n",
      "2489        1\n",
      "1411        1\n",
      "Name: count, Length: 92, dtype: int64\n",
      "hours-per-week\n",
      "40    15216\n",
      "50     2819\n",
      "45     1824\n",
      "60     1475\n",
      "35     1297\n",
      "      ...  \n",
      "82        1\n",
      "94        1\n",
      "92        1\n",
      "74        1\n",
      "87        1\n",
      "Name: count, Length: 94, dtype: int64\n",
      "native-country\n",
      "United-States                 29169\n",
      "Mexico                          643\n",
      "?                               583\n",
      "Philippines                     198\n",
      "Germany                         137\n",
      "Canada                          121\n",
      "Puerto-Rico                     114\n",
      "El-Salvador                     106\n",
      "India                           100\n",
      "Cuba                             95\n",
      "England                          90\n",
      "Jamaica                          81\n",
      "South                            80\n",
      "China                            75\n",
      "Italy                            73\n",
      "Dominican-Republic               70\n",
      "Vietnam                          67\n",
      "Guatemala                        64\n",
      "Japan                            62\n",
      "Poland                           60\n",
      "Columbia                         59\n",
      "Taiwan                           51\n",
      "Haiti                            44\n",
      "Iran                             43\n",
      "Portugal                         37\n",
      "Nicaragua                        34\n",
      "Peru                             31\n",
      "France                           29\n",
      "Greece                           29\n",
      "Ecuador                          28\n",
      "Ireland                          24\n",
      "Hong                             20\n",
      "Cambodia                         19\n",
      "Trinadad&Tobago                  19\n",
      "Laos                             18\n",
      "Thailand                         18\n",
      "Yugoslavia                       16\n",
      "Outlying-US(Guam-USVI-etc)       14\n",
      "Honduras                         13\n",
      "Hungary                          13\n",
      "Scotland                         12\n",
      "Holand-Netherlands                1\n",
      "Name: count, dtype: int64\n",
      "income\n",
      "<=50K    24719\n",
      ">50K      7841\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for col in data.columns:\n",
    "    print(data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9dfb479a-d9b6-49d9-a4fd-2854a6cd6583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of samples with missing values as a '?': 2399\n"
     ]
    }
   ],
   "source": [
    "# if rows with missing values are > x% we need to do something with them\n",
    "# if rows with missing values are <x% we drop ##\n",
    "\n",
    "#filling missing values:\n",
    "# data['age'].value_counts() # filtered from greatest to least value occurence (dropna=False) as a parameter would should how many missing \n",
    "\n",
    "#fillna(value, inplace) to change missing values to the value specified\n",
    "# can you call a function, like knn as the value? \n",
    "# inplace would commit the action to the dataframe , make the change in place\n",
    "# what is x?\n",
    "# what do we do with those rows if %>x\n",
    "\n",
    "rows_with_question = []\n",
    "\n",
    "for index, sample in data.iterrows():\n",
    "    if \"?\" in sample.values:\n",
    "        rows_with_question.append(index)\n",
    "print(\"# of samples with missing values as a '?':\",len(rows_with_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06a0e20d-4052-4b80-b4c3-ecfde66b874c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07367936117936118"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows_with_question) / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5468524-b680-4398-9b7f-4648a0215741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With only 7.36% of the samples having a missing value, we will just drop those\n",
    "# The dataset currently isn't balanced in the target / label feature column\n",
    "# =<50k has roughly 3x the number of samples than >50k\n",
    "# But we can select a balanced mix of either, 15k total samples still, and we will get a lot of variance in the =<50k bootstraps between trees\n",
    "# if all 7.5% of the samples were in the lower sample count category of your label, it may be worth considering value replacement for them\n",
    "# this would be important if you didn't have enough samples in that lesser value count label category\n",
    "# Can you ever just treat ? as a category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c85453e-4937-4e82-bc62-02bd265fba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_Data = data.drop(index=rows_with_question)\n",
    "# for col in cleaned_Data.columns:\n",
    "#     print(cleaned_Data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19a8cd3e-34ee-477f-9f4d-b94c796181bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age               0\n",
      "workclass         0\n",
      "fnlwgt            0\n",
      "education         0\n",
      "education-num     0\n",
      "marital-status    0\n",
      "occupation        0\n",
      "relationship      0\n",
      "race              0\n",
      "sex               0\n",
      "capital-gain      0\n",
      "capital-loss      0\n",
      "hours-per-week    0\n",
      "native-country    0\n",
      "income            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "question_marks_cleaned = cleaned_Data.apply(lambda col: (col == '?').sum())\n",
    "\n",
    "print (question_marks_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70800b7b-9e09-47ae-885b-321cfd9f3e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>284582</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>160187</td>\n",
       "      <td>9th</td>\n",
       "      <td>5</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>209642</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>45781</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>14084</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42</td>\n",
       "      <td>Private</td>\n",
       "      <td>159449</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>5178</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>280464</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>141297</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>India</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>23</td>\n",
       "      <td>Private</td>\n",
       "      <td>122272</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>205019</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>245487</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Amer-Indian-Eskimo</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age         workclass  fnlwgt     education  education-num  \\\n",
       "0    50  Self-emp-not-inc   83311     Bachelors             13   \n",
       "1    38           Private  215646       HS-grad              9   \n",
       "2    53           Private  234721          11th              7   \n",
       "3    28           Private  338409     Bachelors             13   \n",
       "4    37           Private  284582       Masters             14   \n",
       "5    49           Private  160187           9th              5   \n",
       "6    52  Self-emp-not-inc  209642       HS-grad              9   \n",
       "7    31           Private   45781       Masters             14   \n",
       "8    42           Private  159449     Bachelors             13   \n",
       "9    37           Private  280464  Some-college             10   \n",
       "10   30         State-gov  141297     Bachelors             13   \n",
       "11   23           Private  122272     Bachelors             13   \n",
       "12   32           Private  205019    Assoc-acdm             12   \n",
       "14   34           Private  245487       7th-8th              4   \n",
       "\n",
       "           marital-status         occupation   relationship  \\\n",
       "0      Married-civ-spouse    Exec-managerial        Husband   \n",
       "1                Divorced  Handlers-cleaners  Not-in-family   \n",
       "2      Married-civ-spouse  Handlers-cleaners        Husband   \n",
       "3      Married-civ-spouse     Prof-specialty           Wife   \n",
       "4      Married-civ-spouse    Exec-managerial           Wife   \n",
       "5   Married-spouse-absent      Other-service  Not-in-family   \n",
       "6      Married-civ-spouse    Exec-managerial        Husband   \n",
       "7           Never-married     Prof-specialty  Not-in-family   \n",
       "8      Married-civ-spouse    Exec-managerial        Husband   \n",
       "9      Married-civ-spouse    Exec-managerial        Husband   \n",
       "10     Married-civ-spouse     Prof-specialty        Husband   \n",
       "11          Never-married       Adm-clerical      Own-child   \n",
       "12          Never-married              Sales  Not-in-family   \n",
       "14     Married-civ-spouse   Transport-moving        Husband   \n",
       "\n",
       "                  race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0                White    Male             0             0              13   \n",
       "1                White    Male             0             0              40   \n",
       "2                Black    Male             0             0              40   \n",
       "3                Black  Female             0             0              40   \n",
       "4                White  Female             0             0              40   \n",
       "5                Black  Female             0             0              16   \n",
       "6                White    Male             0             0              45   \n",
       "7                White  Female         14084             0              50   \n",
       "8                White    Male          5178             0              40   \n",
       "9                Black    Male             0             0              80   \n",
       "10  Asian-Pac-Islander    Male             0             0              40   \n",
       "11               White  Female             0             0              30   \n",
       "12               Black    Male             0             0              50   \n",
       "14  Amer-Indian-Eskimo    Male             0             0              45   \n",
       "\n",
       "   native-country income  \n",
       "0   United-States  <=50K  \n",
       "1   United-States  <=50K  \n",
       "2   United-States  <=50K  \n",
       "3            Cuba  <=50K  \n",
       "4   United-States  <=50K  \n",
       "5         Jamaica  <=50K  \n",
       "6   United-States   >50K  \n",
       "7   United-States   >50K  \n",
       "8   United-States   >50K  \n",
       "9   United-States   >50K  \n",
       "10          India   >50K  \n",
       "11  United-States  <=50K  \n",
       "12  United-States  <=50K  \n",
       "14         Mexico  <=50K  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_data = cleaned_Data.copy()\n",
    "encoding_data.head(14) # our first removed samples was at index 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da373bb0-f090-49ad-979c-bf03045448ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data[\"income\"] = data[\"income\"].map( {'<=50K': 0, '>50K': 1} ) mapping didnt work for me for some reason, loc issues?\n",
    "# data[\"income\"].isna().sum() \n",
    "\n",
    "# manual binary encoding for just the two categories of the label\n",
    "\n",
    "for index, sample in encoding_data.iterrows():\n",
    "    if sample['income'] == '>50K':\n",
    "        encoding_data.at[index, 'income'] = 1\n",
    "    elif sample['income'] == '<=50K':\n",
    "        encoding_data.at[index, 'income'] = 0\n",
    "encoding_data[\"income\"].isna().sum() \n",
    "encoding_data['income'] = encoding_data['income'].astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d2088ba-38ad-4cdc-b391-6ebf5c41a852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "32555    0\n",
       "32556    1\n",
       "32557    0\n",
       "32558    0\n",
       "32559    1\n",
       "Name: income, Length: 30161, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_data['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de4b3746-361b-4613-9c3d-fc15d643812b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income\n",
       "0    22653\n",
       "1     7508\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_data['income'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "646fa4f4-033c-4b7f-8fd0-7f0726b9df90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income\n",
       "<=50K    22653\n",
       ">50K      7508\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_Data['income'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1132e86-1351-4ea1-9dbb-e3fc67a4335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a679a5a-05bf-4f41-9903-9dcfca106add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8b1d8e1-3cb5-44d2-958d-98d9df8d881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this encoding worked as the income categories were assigned the correct binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2656f472-84f7-4544-8a11-9386caec5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in data and the col\n",
    "\n",
    "# create a vector for each sample for that categorical value - maybe not this, different application\n",
    "# fill it with zeros, and then append a 1 if for the categorical value the samples value was equal to that column value, if not a zero\n",
    "\n",
    "\n",
    "# not sure if we can use sklearn encoding\n",
    "# i am not sure one hot is the best for this because of the added features making the # of features more sparse\n",
    "# But I know that one-hot is the most common encoding even with making features sparse\n",
    "# 15 to nearly 110 level of increase\n",
    "\n",
    "def one_hot (encoding_data, current_col):\n",
    "    replacement_one_hot_cols = pd.DataFrame(index = encoding_data.index)\n",
    "\n",
    "    categories = encoding_data[current_col].unique()\n",
    "    \n",
    "    for category in categories:\n",
    "        print(current_col, \"-\", category)\n",
    "        new_col_name = f\"{current_col}_{category}\"\n",
    "        replacement_one_hot_cols[new_col_name] = (encoding_data[current_col] == category).astype(int)\n",
    "    # print \n",
    "    return replacement_one_hot_cols\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e09a4338-c45f-4973-a0bd-e6c6b5c2fe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass - Self-emp-not-inc\n",
      "workclass - Private\n",
      "workclass - State-gov\n",
      "workclass - Federal-gov\n",
      "workclass - Local-gov\n",
      "workclass - Self-emp-inc\n",
      "workclass - Without-pay\n",
      "education - Bachelors\n",
      "education - HS-grad\n",
      "education - 11th\n",
      "education - Masters\n",
      "education - 9th\n",
      "education - Some-college\n",
      "education - Assoc-acdm\n",
      "education - 7th-8th\n",
      "education - Doctorate\n",
      "education - Assoc-voc\n",
      "education - Prof-school\n",
      "education - 5th-6th\n",
      "education - 10th\n",
      "education - Preschool\n",
      "education - 12th\n",
      "education - 1st-4th\n",
      "marital-status - Married-civ-spouse\n",
      "marital-status - Divorced\n",
      "marital-status - Married-spouse-absent\n",
      "marital-status - Never-married\n",
      "marital-status - Separated\n",
      "marital-status - Married-AF-spouse\n",
      "marital-status - Widowed\n",
      "occupation - Exec-managerial\n",
      "occupation - Handlers-cleaners\n",
      "occupation - Prof-specialty\n",
      "occupation - Other-service\n",
      "occupation - Adm-clerical\n",
      "occupation - Sales\n",
      "occupation - Transport-moving\n",
      "occupation - Farming-fishing\n",
      "occupation - Machine-op-inspct\n",
      "occupation - Tech-support\n",
      "occupation - Craft-repair\n",
      "occupation - Protective-serv\n",
      "occupation - Armed-Forces\n",
      "occupation - Priv-house-serv\n",
      "relationship - Husband\n",
      "relationship - Not-in-family\n",
      "relationship - Wife\n",
      "relationship - Own-child\n",
      "relationship - Unmarried\n",
      "relationship - Other-relative\n",
      "race - White\n",
      "race - Black\n",
      "race - Asian-Pac-Islander\n",
      "race - Amer-Indian-Eskimo\n",
      "race - Other\n",
      "sex - Male\n",
      "sex - Female\n",
      "native-country - United-States\n",
      "native-country - Cuba\n",
      "native-country - Jamaica\n",
      "native-country - India\n",
      "native-country - Mexico\n",
      "native-country - Puerto-Rico\n",
      "native-country - Honduras\n",
      "native-country - England\n",
      "native-country - Canada\n",
      "native-country - Germany\n",
      "native-country - Iran\n",
      "native-country - Philippines\n",
      "native-country - Poland\n",
      "native-country - Columbia\n",
      "native-country - Cambodia\n",
      "native-country - Thailand\n",
      "native-country - Ecuador\n",
      "native-country - Laos\n",
      "native-country - Taiwan\n",
      "native-country - Haiti\n",
      "native-country - Portugal\n",
      "native-country - Dominican-Republic\n",
      "native-country - El-Salvador\n",
      "native-country - France\n",
      "native-country - Guatemala\n",
      "native-country - Italy\n",
      "native-country - China\n",
      "native-country - South\n",
      "native-country - Japan\n",
      "native-country - Yugoslavia\n",
      "native-country - Peru\n",
      "native-country - Outlying-US(Guam-USVI-etc)\n",
      "native-country - Scotland\n",
      "native-country - Trinadad&Tobago\n",
      "native-country - Greece\n",
      "native-country - Nicaragua\n",
      "native-country - Vietnam\n",
      "native-country - Hong\n",
      "native-country - Ireland\n",
      "native-country - Hungary\n",
      "native-country - Holand-Netherlands\n"
     ]
    }
   ],
   "source": [
    "for col in encoding_data.columns:\n",
    "    current_data_type = encoding_data[col].dtype\n",
    "    if current_data_type != 'int64' and current_data_type != 'float64':\n",
    "        add_these = one_hot(encoding_data, col)\n",
    "        \n",
    "        encoding_data = pd.concat([encoding_data, add_these], axis = 1) # make it remove the current col and add the news ones in the same place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3166d61-0295-4323-adde-3cc313dd5371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_column_name = ['income']\n",
    "cols_categorical.remove(label_column_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "944d1c83-9ba5-495e-bcc5-b375531f527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_data = encoding_data.drop(cols_categorical, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e236efc3-b96d-4e96-a455-61dc3d8f849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in encoding_data.columns:\n",
    "#     print(encoding_data[col].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5616b0c-012a-4577-a52c-2aebdff30a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age : int64\n",
      "fnlwgt : int64\n",
      "education-num : int64\n",
      "capital-gain : int64\n",
      "capital-loss : int64\n",
      "hours-per-week : int64\n",
      "income : int64\n",
      "workclass_Self-emp-not-inc : int64\n",
      "workclass_Private : int64\n",
      "workclass_State-gov : int64\n",
      "workclass_Federal-gov : int64\n",
      "workclass_Local-gov : int64\n",
      "workclass_Self-emp-inc : int64\n",
      "workclass_Without-pay : int64\n",
      "education_Bachelors : int64\n",
      "education_HS-grad : int64\n",
      "education_11th : int64\n",
      "education_Masters : int64\n",
      "education_9th : int64\n",
      "education_Some-college : int64\n",
      "education_Assoc-acdm : int64\n",
      "education_7th-8th : int64\n",
      "education_Doctorate : int64\n",
      "education_Assoc-voc : int64\n",
      "education_Prof-school : int64\n",
      "education_5th-6th : int64\n",
      "education_10th : int64\n",
      "education_Preschool : int64\n",
      "education_12th : int64\n",
      "education_1st-4th : int64\n",
      "marital-status_Married-civ-spouse : int64\n",
      "marital-status_Divorced : int64\n",
      "marital-status_Married-spouse-absent : int64\n",
      "marital-status_Never-married : int64\n",
      "marital-status_Separated : int64\n",
      "marital-status_Married-AF-spouse : int64\n",
      "marital-status_Widowed : int64\n",
      "occupation_Exec-managerial : int64\n",
      "occupation_Handlers-cleaners : int64\n",
      "occupation_Prof-specialty : int64\n",
      "occupation_Other-service : int64\n",
      "occupation_Adm-clerical : int64\n",
      "occupation_Sales : int64\n",
      "occupation_Transport-moving : int64\n",
      "occupation_Farming-fishing : int64\n",
      "occupation_Machine-op-inspct : int64\n",
      "occupation_Tech-support : int64\n",
      "occupation_Craft-repair : int64\n",
      "occupation_Protective-serv : int64\n",
      "occupation_Armed-Forces : int64\n",
      "occupation_Priv-house-serv : int64\n",
      "relationship_Husband : int64\n",
      "relationship_Not-in-family : int64\n",
      "relationship_Wife : int64\n",
      "relationship_Own-child : int64\n",
      "relationship_Unmarried : int64\n",
      "relationship_Other-relative : int64\n",
      "race_White : int64\n",
      "race_Black : int64\n",
      "race_Asian-Pac-Islander : int64\n",
      "race_Amer-Indian-Eskimo : int64\n",
      "race_Other : int64\n",
      "sex_Male : int64\n",
      "sex_Female : int64\n",
      "native-country_United-States : int64\n",
      "native-country_Cuba : int64\n",
      "native-country_Jamaica : int64\n",
      "native-country_India : int64\n",
      "native-country_Mexico : int64\n",
      "native-country_Puerto-Rico : int64\n",
      "native-country_Honduras : int64\n",
      "native-country_England : int64\n",
      "native-country_Canada : int64\n",
      "native-country_Germany : int64\n",
      "native-country_Iran : int64\n",
      "native-country_Philippines : int64\n",
      "native-country_Poland : int64\n",
      "native-country_Columbia : int64\n",
      "native-country_Cambodia : int64\n",
      "native-country_Thailand : int64\n",
      "native-country_Ecuador : int64\n",
      "native-country_Laos : int64\n",
      "native-country_Taiwan : int64\n",
      "native-country_Haiti : int64\n",
      "native-country_Portugal : int64\n",
      "native-country_Dominican-Republic : int64\n",
      "native-country_El-Salvador : int64\n",
      "native-country_France : int64\n",
      "native-country_Guatemala : int64\n",
      "native-country_Italy : int64\n",
      "native-country_China : int64\n",
      "native-country_South : int64\n",
      "native-country_Japan : int64\n",
      "native-country_Yugoslavia : int64\n",
      "native-country_Peru : int64\n",
      "native-country_Outlying-US(Guam-USVI-etc) : int64\n",
      "native-country_Scotland : int64\n",
      "native-country_Trinadad&Tobago : int64\n",
      "native-country_Greece : int64\n",
      "native-country_Nicaragua : int64\n",
      "native-country_Vietnam : int64\n",
      "native-country_Hong : int64\n",
      "native-country_Ireland : int64\n",
      "native-country_Hungary : int64\n",
      "native-country_Holand-Netherlands : int64\n"
     ]
    }
   ],
   "source": [
    "for col in encoding_data.columns:\n",
    "    current_data_type = encoding_data[col].dtype\n",
    "    print(col,':',current_data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd666e2a-624a-4db1-94f9-a794337c51a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb3ecb-afcb-42d2-999f-7058832acb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#22653 less than 50k\n",
    "#7508 more than 50k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6e1e3-b9ec-4492-978a-161f743d342a",
   "metadata": {},
   "source": [
    "# SKLEARN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0a933-1b5b-4e87-8c80-fa6434789a50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)[source]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=10, min_samples_leaf=5, max_features='sqrt', bootstrap=True, oob_score=False, n_jobs=-1, random_state=110)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data[['income']]\n",
    "y_fixed = encoder.fit_transform(y)\n",
    "# rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e060b5-73a4-4bb4-9b22-8a8aabd62031",
   "metadata": {},
   "source": [
    "####Lab 1-2 Stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c30e24-6d87-41ee-a53c-aa6f047ae53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_information (num_samples):\n",
    "    depth_counter = 0\n",
    "    resulting = 15\n",
    "    calcd_resulting = num_samples\n",
    "    while (calcd_resulting > resulting):\n",
    "        depth_counter+=1\n",
    "        calcd_resulting = np.ceil(calcd_resulting / 2) #int division \"//\" would do floor\n",
    "\n",
    "    return depth_counter, calcd_resulting\n",
    "\n",
    "\n",
    "\n",
    "#hpyer parameters\n",
    "label_col_name = \"income\"\n",
    "num_trees_in_forest = 150\n",
    "num_random_features = int(np.ceil(np.sqrt(data.shape[1])))\n",
    "# depth_threshold = 10 #32560 samples / 2 (10 times) = 32\n",
    "# we could use a for loop to calc this based on sample size, and pass that to the two below \n",
    "# split_leaf_minimum = 32\n",
    "\n",
    "\n",
    "depth_threshold, split_leaf_minimum = sample_information(data.shape[0])\n",
    "split_node_min_samples = (split_leaf_minimum * 2) + 1\n",
    "\n",
    "\n",
    "\n",
    "print(depth_threshold, split_leaf_minimum, split_node_min_samples)\n",
    "# what is a good depth? what is a good number of min samples to split and per leaf values?\n",
    "# i think i read 6-7 depth is a lot\n",
    "\n",
    "# how many combinations of num random features are possible, so how many times within the num trees in forest will there be duplicates?\n",
    "# how many duplicates would be good mathematically?\n",
    "# 15 choose 4 is 1365, so what % of this do we want for num trees in forest? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc5a01-ba19-4955-8678-b260c83fb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this function I am taking in the training dataset, the label and the total number of train samples\n",
    "# for each training sample in the dataset we randomly grab a row from the training dataset to create our bootstrap dataset\n",
    "# so in our case we have a training dataset of 8500, so we create a bootstrap that has 8500 samples from our original train dataset\n",
    "# we get indices to pull from the training set randomly by generating a random # from 0 to 8499 and adding that row to our array\n",
    "# once we have created our array of samples from the rows of the training dataset, we create the bootstrap dataset from our array of rows\n",
    "# this method will allow duplicates, and with this many samples we should end up with roughly 65% of the train dataset in our bootstrap\n",
    "# then we seperate the label column from the X_bootstrap resulting in just the training data with no label\n",
    "# then y_bootstrap is just the labels column, but all of the indices match up\n",
    "\n",
    "def bootstrap_creator(this_tree_data, label_column_name, total_samples):\n",
    "  bootstrap_for_this_tree = [] # holder array\n",
    "\n",
    "  for i in range(total_samples): # match size of bootstrap to size of training data set\n",
    "    random_index_to_add = np.random.randint(0 , total_samples) # generate a row index to grab from the training datatset\n",
    "    bootstrap_for_this_tree.append(this_tree_data.iloc[random_index_to_add]) # add that dataset row to our array\n",
    "\n",
    "\n",
    "    #create the bootstrap dataframe\n",
    "  bootstrap = pd.DataFrame(bootstrap_for_this_tree) # I suppose I could have np.unique before creating the dataframe here - nevermind that didn't work how I intended\n",
    "\n",
    "    # generate X and Y bootstraps from bootstrap\n",
    "  X_bootstrap = bootstrap.drop(label_column_name, axis=1)\n",
    "  y_bootstrap = bootstrap[label_column_name]\n",
    "\n",
    "  return X_bootstrap, y_bootstrap, bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfefa49-7fd3-4581-81b2-bc583ba467ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used to calculate the rows from the training dataset that did not make it into this bootstrap dataset\n",
    "# we pass in the label, the training dataset and the randomly selected bootstrap dataset\n",
    "# with a dataframe this is a bit simpler than with the all integer numpy array from the class example\n",
    "# here we can simply drop all of the indices (rows) that exist in the bootstrap data set from the training data set and we are left with the out of box sample set\n",
    "# I didn't need to copy the dataframe, I find myself doing this somewhat often when I am working with dataframes to maintain the last version of them\n",
    "# I want to say I picked up this habit during my AI course but I don't remember why exactly, I think it had to do with model evaluations\n",
    "# then to find the data frames for X_oob and y_oob we simply drop the label from our total oob dataframe for X and pull out only the label column for our y\n",
    "\n",
    "def oob_determination(tree_data, bootstrap, label_column_name):\n",
    "\n",
    "    # drop bootstrap rows from the total tree training data set to be left with a dataframe of only the none-selected training samples\n",
    "  oob_for_this_tree = tree_data.drop(index=bootstrap.index) # drop all the indices from the tree dataset/ training dataset that exist within the bootstrap for this specific tree\n",
    "\n",
    "    # copy oob_for_this_tree incase we need to pass it as a return later\n",
    "  all_oob_this_tree = oob_for_this_tree.copy()\n",
    "\n",
    "    # make the X-oob and y_oob respective dataframes\n",
    "  X_oob = oob_for_this_tree.drop(label_column_name, axis=1)\n",
    "  y_oob = oob_for_this_tree[label_column_name]\n",
    "\n",
    "  return X_oob, y_oob, all_oob_this_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f3947-8e0e-48d0-917e-88d0761fadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calls the bootstrap creation function and then passes the bootstrap dataset into oob to return the oob values\n",
    "# meaning bootstrap + oob below make up the entirety of the 8500 data samples within sample_data_train without any overlap\n",
    "# and this function just returns the output of both lumped together\n",
    "def boot_strap_and_oob(tree_data, label_column_name, total_samples):\n",
    "  X_bootstrap, y_bootstrap, bootstrap = bootstrap_creator(tree_data, label_column_name, total_samples)\n",
    "  X_oob, y_oob, all_oob = oob_determination(tree_data, bootstrap, label_column_name)\n",
    "\n",
    "  return X_bootstrap, y_bootstrap, X_oob, y_oob, bootstrap, all_oob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28fa17-c7ac-494d-a61d-326a378ebdad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7790e7d-9b5e-43bc-9631-1e0e5954e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the split point function that we developed in class and used in assignment 1\n",
    "# when we call it in our program we are iterating through a series of values to determine all splitpoints for the feature\n",
    "\n",
    "def split_point_finder(x):\n",
    "  mid_points_in_column = []\n",
    "  x_sorted = sorted(x)\n",
    "\n",
    "  for i in range(0, len(x)-1):\n",
    "    mid_point = ( (x_sorted[i] + x_sorted[i+1]) / 2)\n",
    "    mid_points_in_column.append(mid_point)\n",
    "  return mid_points_in_column\n",
    "\n",
    "\n",
    "# lab 1 below, 2 above\n",
    "\n",
    "# x here is a single column, we create a list to store the midpoints (len column -1 values)\n",
    "# we sort that column, and for every value from 0 to len -1 we take the average of the current column value and the next\n",
    "# we append these midpoints to the list and return that list, giving us a list of midpoints per column\n",
    "\n",
    "def split_point_finder(x):\n",
    "  mid_points_in_column = []\n",
    "  x_sorted = sorted(x)\n",
    "\n",
    "  for i in range(0, len(x)-1):\n",
    "    mid_point = ( (x_sorted[i] + x_sorted[i+1]) / 2)\n",
    "    mid_points_in_column.append(mid_point)\n",
    "  return mid_points_in_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3082584-00bf-4bae-9300-9343dbcaf543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the left and right node function we developed in class and used in assignment 1\n",
    "# we use this to find the x-y pairs that will go to the left and ride nodes given a certain split point, we test all unique split points with this later\n",
    "\n",
    "def branching_splits(x, y, split_point):\n",
    "  mask = x >= split_point\n",
    "  anti_mask = x < split_point\n",
    "  x_right = x[mask]\n",
    "  x_left = x[anti_mask]\n",
    "  y_right = y[mask]\n",
    "  y_left = y[anti_mask]\n",
    "  return x_right, x_left, y_right, y_left\n",
    "\n",
    "#1 below 2 above\n",
    "\n",
    "# here we pass in the feature column, the label column and the split point we are using\n",
    "# the mask applied filters values greater or equal to the split point\n",
    "# the anti mask is values less than the split point\n",
    "# we then create true/false or right/left lists of the feature column and the label column with the mask filtering\n",
    "# we return the the true/false or right/left list of the feature column and the corresponding label column\n",
    "\n",
    "def branching_splits(x, y, split_point):\n",
    "  mask = x >= split_point\n",
    "  anti_mask = x < split_point\n",
    "  x_right = x[mask]\n",
    "  x_left = x[anti_mask]\n",
    "  y_right = y[mask]\n",
    "  y_left = y[anti_mask]\n",
    "  return x_right, x_left, y_right, y_left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3304366-6b63-4f4c-aaa7-06772ce9c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we iterate through each feature in dataframe features except for our label column, which is price, but I made it declarable in the function call\n",
    "# we seperate the label data from the rest of the dataframe and then iterate over the remaining columns which are the features\n",
    "# we find the split points for each column, there are many repeats so we make a list of just the unique split point values\n",
    "# I then iterate through those unique split points getting the left and right branching values that are then sent to the MAE function\n",
    "# the mae function calculates the root node MAE and the weighted mae of the leaf nodes for that split point being tested\n",
    "# after getting the MAE values back we store them in a dictionary with this structure key:(feature, split point), value:(root mae, split point mae)\n",
    "# we return the columns we iterated through and the dictionary with the unique split points for each feature and the value pair for the root mae and that split points weighted mae\n",
    "\n",
    "#lab 1\n",
    "\n",
    "def best_branching_multiple_features(data, label_column_name):\n",
    "\n",
    "  y = data[label_column_name]\n",
    "  features_to_test = data.drop(label_column_name, axis=1)\n",
    "  number_features = len(features_to_test.columns)\n",
    "\n",
    "  feature_error_dictionary = {}\n",
    "\n",
    "\n",
    "  for feature in features_to_test.columns:\n",
    "    x = data[feature]\n",
    "\n",
    "    split_points = split_point_finder(x)\n",
    "    split_points_unique = np.unique(split_points)\n",
    "\n",
    "    for split_point in split_points_unique:\n",
    "      x_right, x_left, y_right, y_left = branching_splits(x, y, split_point)\n",
    "      unique_errors = MAE(y,y_right,y_left)\n",
    "      feature_error_dictionary[feature, split_point] = [unique_errors]\n",
    "\n",
    "  return features_to_test.columns, feature_error_dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# I created this function instead of of adding this to the best_branding_multiple_features functions because I felt like the function was too long and harder to understand\n",
    "# this function calls the best_branching_multiple_features function to retrieve the columns we iterated over and the dictionary of split point/ weighted mae values\n",
    "# the goal here is to iterate through the dictionary values for each feature column and find the key/value pair with the lowest weighted mae and record it\n",
    "# This could be implemented into the \" for feature in features_to_test.columns:\" statement in the function that creates the dictionary but I found it more difficult to comprehend\n",
    "# that may be personal prefence though, this adds some computational overhead having to re-iterate through all of the features again, so for very large datasets it would be inefficient\n",
    "# for the upcoming sections we need a way to retrieve the best feature, its split and the weighted mae for any node of a tree so I have chosen to extract those values individually\n",
    "# instead of only bundled in a key,value pair - I am also returning a dictionary of key,value pairs for all of the feature columns incase we need to use a specific one even if its not the best\n",
    "# I iterate through all of the features, within those features we check the dictionary to see if the entry belongs to that feature\n",
    "# if it does then we check to see if that weighted MAE is the lowest so far for that feature, if it is we save it, if not we move on\n",
    "# we do this for each feature, and we also check to see if it is the lowest recorded MAE amongst all features by saving the lowest MAE key,value pair for each feature\n",
    "# after each feature is iterated its lowest MAE key/value is evaluated against the others and the lowest is recorded as the overall best feature, split point and MAE\n",
    "\n",
    "\n",
    "def current_branching_best(data, label_column_name):\n",
    "  tested_features, feature_error_dictionary = best_branching_multiple_features(data, label_column_name)\n",
    "\n",
    "  each_feature_best_split_point = {}\n",
    "  best_overall_feature,best_overall_split_for_feature,best_overall_error_for_feature = None, None, float('inf')\n",
    "\n",
    "  for feature in tested_features:\n",
    "    best_error_for_current_feature, best_split_for_current_feature = float('inf'), None\n",
    "\n",
    "    for (this_feature, this_split_point), error_value_pair in feature_error_dictionary.items():\n",
    "\n",
    "      if this_feature == feature:\n",
    "        error_pair = error_value_pair[0]\n",
    "        split_mae = error_pair[1]\n",
    "\n",
    "        if split_mae < best_error_for_current_feature:\n",
    "          best_error_for_current_feature = split_mae\n",
    "          best_split_for_current_feature = this_split_point\n",
    "    each_feature_best_split_point[feature, best_split_for_current_feature] =(error_pair[0], best_error_for_current_feature)\n",
    "\n",
    "    if best_error_for_current_feature < best_overall_error_for_feature:\n",
    "      best_overall_error_for_feature = best_error_for_current_feature\n",
    "      best_overall_feature = feature\n",
    "      best_overall_split_for_feature = best_split_for_current_feature\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  return best_overall_feature, best_overall_split_for_feature, best_overall_error_for_feature, each_feature_best_split_point\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f656d7-17dc-4989-b5c3-bb305dd92501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gini\n",
    "def Gini(count_series, samples):\n",
    "    return 1 - sum( ((count / samples) ** 2) for count in count_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1eb374-5594-4252-8f7d-25b217d40db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entropy\n",
    "def Entropy(count_series, samples):\n",
    "    return -sum( ((count / samples) * np.log2((count / samples))) for count in count_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89b4dd-8cba-4e93-b3b9-511f780a046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_Entropy(y, y_right, y_left):\n",
    "    num_samples_root = len(y)\n",
    "    num_samples_left_child = len(y_left)\n",
    "    num_samples_right_child = len(y_right)\n",
    "\n",
    "    root_entropy = Entropy( y.value_counts(), num_samples_root)\n",
    "\n",
    "    left_child_entropy = Entropy( y_left.value_counts(), num_samples_left_child)\n",
    "\n",
    "    right_child_entropy = Entropy ( y_right.value_counts(), num_samples_right_child)\n",
    "\n",
    "    weighted_nodes_entropy = (num_samples_left_child / num_samples_root) * left_child_entropy + (num_samples_right_child / num_samples_root) * right_child_entropy\n",
    "    \n",
    "\n",
    "    return root_entropy, weighted_nodes_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643725b-ed03-41dc-b85e-582453989aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_Gini(y, y_right, y_left):\n",
    "    y_counts = y.value_counts()\n",
    "    num_samples_root = len(y)\n",
    "    num_samples_left_child = len(y_left)\n",
    "    num_samples_right_child = len(y_right)\n",
    "\n",
    "    root_gini = Gini( y.value_counts(),  num_samples_root)\n",
    "\n",
    "    left_child_gini = Gini( y_left.value_counts(), num_samples_left_child)\n",
    "\n",
    "    right_child_gini = Gini ( y_right.value_counts(), num_samples_right_child)\n",
    "\n",
    "    weighted_nodes_gini = (num_samples_left_child / num_samples_root) * left_child_gini + (num_samples_right_child / num_samples_root) * right_child_gini\n",
    "    \n",
    "\n",
    "    return root_gini, weighted_nodes_gini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa0349-f074-478c-ab72-9d44e6d859b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wegi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1dd670-ddf0-4210-86ab-03d8c01be9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is pulled from assignment 1 and has a few slight modifications\n",
    "# this functions purpose is to find the best split point and feature at each node of the decision tree\n",
    "# it then returns the feature, split point, root node mse, branching leaf node weighted mse and the dictionary of all errors for the features tested\n",
    "\n",
    "# what we have modified in this code is the ability to randomly select features at each node of the tree, a subset of the total training dataset features\n",
    "# we do this by taking num_random_features_to_choose making it a parameter that can be adjusted in our decision tree creation function\n",
    "# we specify the number of features we want to randomly choose, then before we begin looping for the feautures looking for the best feature / split point\n",
    "# we randomly select from our total features in the dataset columns the parameter number of features to test for the splitting at this node\n",
    "# so if we have a parameter of 5 and 17 feature columns in our dataframe, we randomly choose 5 of those 17 with duplicates not being allowed\n",
    "# from there the function works the same way as assignment 1 where we determine the best split point and feature from those we test to determine the best splitting for this node\n",
    "\n",
    "# we pass in the bootstrap data to this function not the total training dataset\n",
    "def best_branching_random_features(data, label_column_name, num_random_features_to_choose, criterion_equation):\n",
    "\n",
    "  y = data[label_column_name]\n",
    "  possible_features_to_test = data.drop(label_column_name, axis=1) # drop the label column from the dataframe so only features are left\n",
    "\n",
    "  randomized_features_this_node = rng.choice(possible_features_to_test.columns, num_random_features_to_choose, replace = False)\n",
    "  # we use rng.choice to randomly choose (parameter number of features) from (all feature columns that are present in the dataframe) and don't allow duplicates\n",
    "\n",
    "  this_node_data = possible_features_to_test[randomized_features_this_node].copy() # then we create a new dataframe only copying the feature columns selected by our random feature selector above\n",
    "  # we get a smaller dataframe with only the number of features declared in the function parameter\n",
    "\n",
    "  feature_error_dictionary = {}\n",
    "\n",
    "  # In assignment 1 we wouldn't have ran into these errors, but I ran into an error for each of these variables not being initialized\n",
    "  # min error should be set to inf, anythign smaller will replace it\n",
    "  minimum_error = float('inf')\n",
    "\n",
    "  # these 3 can create errors further down our decision tree building if no split points are found to reduce the minimum error, if they arent initialized now and we go to return them\n",
    "  # then we get the errors I pasted in below - if your testing a split with only has 100 samples and for a feature they are all the same value, then no split point exists and these\n",
    "  # values would be left un-initialized given our structure below\n",
    "  node_mse = float('inf')\n",
    "  best_feature = 'Kelton'\n",
    "  best_split_point_current_feature = 992018683\n",
    "  #UnboundLocalError: cannot access local variable 'best_feature' where it is not associated with a value\n",
    "  #UnboundLocalError: cannot access local variable 'node_mse' where it is not associated with a value\n",
    "  #UnboundLocalError: cannot access local variable 'best_split_point_current_feature' where it is not associated with a value\n",
    "\n",
    "  for feature in this_node_data.columns:\n",
    "    x = data[feature]\n",
    "\n",
    "    split_points = split_point_finder(x)\n",
    "    split_points_unique = np.unique(split_points)\n",
    "\n",
    "    for split_point in split_points_unique:\n",
    "      x_right, x_left, y_right, y_left = branching_splits(x, y, split_point)\n",
    "\n",
    "      match criterion_equation:\n",
    "        case 'Gini':\n",
    "            unique_errors = Gini(y,y_right,y_left)\n",
    "            feature_error_dictionary[feature, split_point] = [unique_errors]\n",
    "        case 'Entropy':\n",
    "            unique_errors = Entropy(y,y_right,y_left)\n",
    "            feature_error_dictionary[feature, split_point] = [unique_errors]\n",
    "        \n",
    "        \n",
    "      # unique_errors = weighted_branch_MSE(y,y_right,y_left)\n",
    "      # feature_error_dictionary[feature, split_point] = [unique_errors]\n",
    "\n",
    "        # recording the best feature and split point based on lowest weighted branch mse from the mse function\n",
    "      if unique_errors[1] < minimum_error:\n",
    "        minimum_error = unique_errors[1]\n",
    "        node_mse = unique_errors[0]\n",
    "        best_feature = feature\n",
    "        best_split_point_current_feature = split_point\n",
    "\n",
    "  return feature_error_dictionary, node_mse, minimum_error, best_feature, best_split_point_current_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423c23a-4abc-46a5-955d-5fd8fd3f3292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201d9eb-d7a6-4573-bc3a-975e921e2bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085a420-7336-419b-9da0-0d97442cdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing that the best_branching_random_features function delivers the best split point, feature, and mse of leaves along with root node mse\n",
    "def main(data, label_column_name, num_randomized_features):\n",
    "  total_samples = data.shape[0]\n",
    "\n",
    "  dictionary_of_errors_for_node, root_mse, branch_mse, feature, split_point = best_branching_random_features(data, label_column_name, num_randomized_features)\n",
    "\n",
    "  print(root_mse)\n",
    "  print(branch_mse)\n",
    "  print(feature)\n",
    "  print(split_point)\n",
    "\n",
    "  return dictionary_of_errors_for_node, root_mse, branch_mse, feature, split_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8d55e-07d0-4038-9097-d7e365f8c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnair_data = data.copy()\n",
    "label_column_name = 'income'\n",
    "# number_of_random_features = int(np.sqrt((mcnair_data.shape[1])-1)) # rounds down to 4\n",
    "total_samples = data.shape[0]\n",
    "X_bootstrap, y_bootstrap, X_oob, y_oob, bootstrap, all_oob = boot_strap_and_oob(mcnair_data, label_column_name, total_samples)\n",
    "\n",
    "\n",
    "dictionary_of_errors_for_node, root_mse, branch_mse, feature, split_point = main(bootstrap, label_column_name, num_random_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c1584-0ba3-4b8f-94d3-42a67c676a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = []\n",
    "for feature, split_point in dictionary_of_errors_for_node:\n",
    "  features.append(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb3e10-047d-4941-9b7f-a1b034b1e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa83268-39a6-4208-b310-2227c8f44faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is my decision tree builder function, which is similar to that of which was built in class but I went with a dictionary instead of a tuple return type\n",
    "# I have set parameters for max depth of the tree, minimum number of samples in a node to try to split it, and a minimum number of samples in the resulting leaves to continue with the splitting\n",
    "\n",
    "\n",
    "def dt_builder(bootstrap_current_node, label_column_name, num_randomized_features, minimum_samples_per_leaf, min_samples_per_split, max_depth, depth = 0):\n",
    "\n",
    "  before_split_sample_size = bootstrap_current_node.shape[0]# get the number of samples in the current parent node\n",
    "\n",
    "\n",
    "  # this is like the max depth checker in the in-class example, but we must also check to make sure that there are enough samples in the parent node to even determine if splitting is worthwhile\n",
    "  if depth >= max_depth or before_split_sample_size < min_samples_per_split: # if depth is >= max depth or there arent enough samples in the parent node then we return a prediction\n",
    "    return {\"prediction_this_node\": np.mean(bootstrap_current_node[label_column_name])}\n",
    "\n",
    "  # we have not reached max depth or the minimum number of samples in the parent node, so we call our function to find the best splitting at this node\n",
    "  # we pass thorugh the current node sample, the label, and the number of features to randomly choose\n",
    "  dictionary_of_errors_for_node, root_mse, branch_mse, feature, split_point = best_branching_random_features(bootstrap_current_node, label_column_name, num_randomized_features)\n",
    "\n",
    "\n",
    "  # this is where I began getting errors for the uninitialized best feature, split and root mse in our branching function\n",
    "  # I have initialized the best feature to my name, if this is returned that means there were no valid split points found in the features randomly selected\n",
    "  # this seems to be due to a small number of samples in the node and the sample feature values being the same, like all having 1.0 floors as an example, no splits found so kelton is returned\n",
    "  # in the case that this non-feature basecase is returned we can no longer split and return this node as a prediction node\n",
    "  if feature == 'Kelton':\n",
    "    # print(feature) # testing\n",
    "    return {\"prediction_this_node\": np.mean(bootstrap_current_node[label_column_name])}\n",
    "\n",
    "  # just like branching splits we use vectorized operations on the current node to get the data for the resulting left and right leaf nodes from the split\n",
    "  # I suppose you could store these in the error dictionary from the best_branching_random_features from above\n",
    "  left_node_data = bootstrap_current_node[bootstrap_current_node[feature] < split_point]\n",
    "\n",
    "  right_node_data = bootstrap_current_node[bootstrap_current_node[feature] >= split_point]\n",
    "\n",
    "  # this gets the size of the resulting split leaf nodes, left and right, from the lead node data sets, we get the sizes to check the leaf node sample sizes against the minimum\n",
    "  left_sample_size = left_node_data.shape[0]\n",
    "  right_sample_size = right_node_data.shape[0]\n",
    "\n",
    "  # here we are checking if both the left and right resulting leaf node samples from splitting are greater than or equal to the minimum value parameter we have set\n",
    "  # if either is less than the parameter threshold we do not do the node split and we return the node as a prediction node instead\n",
    "  if left_sample_size < minimum_samples_per_leaf or right_sample_size < minimum_samples_per_leaf:\n",
    "    return {\"prediction_this_node\": np.mean(bootstrap_current_node[label_column_name])}\n",
    "\n",
    " # these are our recrusive calls to this function, for the left nodes and right nodes, each time we increase the depth by 1 so we know if we need to stop the branching due to depth\n",
    " # these calls are what create the tree leaf structure below the root node which is created the first time we call this function\n",
    "  left_leaf_node = dt_builder(left_node_data, label_column_name, num_randomized_features, minimum_samples_per_leaf, min_samples_per_split, max_depth, depth + 1)\n",
    "  right_leaf_node = dt_builder(right_node_data, label_column_name, num_randomized_features, minimum_samples_per_leaf, min_samples_per_split, max_depth, depth + 1)\n",
    "\n",
    "  # just like our tuple return from the in class version of this code, we return the feature, the split point, the left leaf node and right leaf node, in dictionary form instead of tuples\n",
    "  return { \"feature\": feature, \"split_point\": split_point, \"left_leaf_node\": left_leaf_node, \"right_leaf_node\": right_leaf_node}\n",
    "  # I had dynamic depth tracking in the key of the leaf nodes, but that was making tree navigation more difficult so I opted to remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7bff36-7e0f-4e20-8050-665a3f2add55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so with the functionality of our decision tree builder and prediction recursive functions established, the random forest regressor and the aggregate prediciton functions are not too difficult\n",
    "# we just need to create loops to build enough trees to become a forest of X size, and then traverse all of them to make predictions with the forest structure\n",
    "# here we need to pass in all of the parameters that we need, the training data, label, number of tree to build, the num of random features and the bootstramp sample size which should match out training sample size\n",
    "\n",
    "def random_forest_regressor_from_scratch(data, label_column_name, number_of_trees, num_randomized_features, bootstrap_sample_size):\n",
    "\n",
    "  forest_of_trees_dict = {}\n",
    "\n",
    "  # these 3 variables are the ones that we change to try and tune our trees which resulting in tuning the forest\n",
    "  # I have been fiddling with these to get lower MSE's in our testing results\n",
    "  # 8500 split values rounded up are 4250, 2125, 1063, 532, 266, 133, 67, 34, 17, 9, 5, 3, 2\n",
    "  # at 13 splits we are getting leafs with very few samples\n",
    "  # I was getting some of my best results at min samples 9, max depth 10 if I recall\n",
    "  minimum_samples_per_leaf = 3\n",
    "  min_samples_per_split = (minimum_samples_per_leaf * 2) + 1 # this is me rounding up when determining how many samples in the current node are needed to even try to split before testing leaf # samples\n",
    "  max_depth = 7\n",
    "\n",
    "  arguments_tuple = (data, label_column_name, num_randomized_features, bootstrap_sample_size, minimum_samples_per_leaf, min_samples_per_split, max_depth)\n",
    "\n",
    "  for i in range(number_of_trees):\n",
    "    arguments_tuple_list = [arguments_tuple]\n",
    "\n",
    "  # with Pool() as pool:\n",
    "  #   resulting_trees = pool.map(single_tree, arguments_tuple_list)\n",
    "\n",
    "  with get_context(\"fork\").Pool() as pool:\n",
    "  # with Pool() as pool:\n",
    "    resulting_trees = pool.map(single_tree, arguments_tuple_list)\n",
    "\n",
    "\n",
    "  #build all the trees paramaterized\n",
    "  for i, (tree, oob_indices) in enumerate(resulting_trees):\n",
    "\n",
    "    # each tree gets its own bootstrap, everytime we call dt_builder we need a new bootstrap\n",
    "    #dt_builder builds trees with randomized features at each node of the tree\n",
    "    # then we store all of the OOB df indices to a list, you could store the whole dataframe, but when I access them later I only need the index values, so I am going to take only those now\n",
    "    # you can use list(df indices) or the method that I used as .tolist() - I do remember seeing one comment on a website stating something about how list(df indices) might be safer or more reliable for some data structures? something to do with numpy I believe\n",
    "    # we then store each tree in a  forest dictionary entry, the key is the tree number and the value is a tuple with value[0] = dictionary that is the tree itself, and value[1] is the array of indices of the oob values for the bootstrap sample used in building the tree\n",
    "\n",
    "    forest_of_trees_dict[i] = (tree, oob_indices)\n",
    "\n",
    "# then we return the forest dictionary\n",
    "  return forest_of_trees_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45358565-a112-4811-9de6-603ad5fac317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a single test decision tree, min samples, min samples to even try to split is at least 2x the minimum samples per leaf\n",
    "# I was trying to experiment with setting the min samples to consider splitting higher than 2x the min samples per leaf\n",
    "# max depth of 10, https://www.geeksforgeeks.org/how-to-choose-ideal-decision-tree-depth-without-overfitting/\n",
    "# in the link they are using a classifier model, so this may be irrelevant to a regression model\n",
    "# but they test depths from 1-15 and their accuraccy seems to almost peak around a max depth of 7\n",
    "# I have been testing between 3-12 and I seem to be getting the best results around 8-12 given the parameter of 4 randomly chosen features I have been using,\n",
    "mcnair_data = data_sample_train.copy()\n",
    "label_column_name = 'price'\n",
    "num_randomized_features = int(np.sqrt((mcnair_data.shape[1]-1)))\n",
    "minimum_samples_per_leaf = 12\n",
    "min_samples_per_split = (minimum_samples_per_leaf * 2) + 1\n",
    "max_depth = 10\n",
    "bootstrap_sample_size = mcnair_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd56214-500a-4b1d-8733-103a9cc4a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X_bootstrap, y_bootstrap, X_oob, y_oob, bootstrap_current_node, all_oob = boot_strap_and_oob(data, label_column_name, bootstrap_sample_size)\n",
    "    individual_tree_dict = dt_builder(bootstrap_current_node, label_column_name, num_randomized_features, minimum_samples_per_leaf, min_samples_per_split, max_depth, depth = 0)\n",
    "    individual_tree_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602edc64-1836-462a-805f-cad6b4e280ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of these functions working correctly, passing in the the training sample number of samples and retreiving all of the bootstrap & oob data frames\n",
    "def main_test_bs(data, label_column_name):\n",
    "  total_samples = data.shape[0]\n",
    "  X_bootstrap, y_bootstrap, X_oob, y_oob, bootstrap, all_oob = boot_strap_and_oob(data, label_column_name, total_samples)\n",
    "  return X_bootstrap, y_bootstrap, X_oob, y_oob, bootstrap, all_oob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688741d-b214-40c6-b39e-9431fd17c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnair_data = data.copy()\n",
    "label_column_name = 'income'\n",
    "\n",
    "X_bootstrap, y_bootstrap, X_oob, y_oob, bootstrap, all_oob = main_test_bs(mcnair_data, label_column_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223cc3a3-0826-4987-bdd4-0108098055fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap.drop_duplicates().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8453ef0-73eb-44e6-b0d4-587abd5142a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_oob.drop_duplicates().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d665a33-2d87-4f1a-9479-53b8cd297265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dee369c-a69b-47f2-a733-92be6540e0af",
   "metadata": {},
   "source": [
    "Working on getting multiprocessing setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53f32a-5852-4308-b49a-d4a191c3bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcnair_rf_classifier import imported_random_forest_classifier\n",
    "from multiprocessing import Pool # multiprocessing but requires function to be in a .py file when working with jupyter?\n",
    "from multiprocessing import get_context #older fork method not newer spawn, issues with this also, need to put functions in .py file maybe\n",
    "\n",
    "# https://docs.python.org/3.10/library/multiprocessing.html\n",
    "# https://stackoverflow.com/questions/72830743/multiprocessing-changed-in-python-3-9-from-3-7-on-macosx-how-to-fix\n",
    "# https://machinelearningmastery.com/multiprocessing-in-python/\n",
    "# https://medium.com/@dialoglk/boosting-performance-and-efficiency-exploring-the-advantages-of-multiprocessing-in-python-fd48202e1107\n",
    "#\n",
    "\n",
    "# def random_forest_regressor_from_scratch(data, label_column_name, number_of_trees, num_randomized_features, bootstrap_sample_size):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee38d6-7315-4f89-8d31-baaf19385df4",
   "metadata": {},
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4bb93-eba1-4de4-9298-42a97f86040c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# need to figure out Criterion for determining the best feature split (Gini impurity or Entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e8e57-1879-4a60-b3ae-420b076e285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_traversal(individual_tree, test_data):\n",
    "# in the in class version with tuples and different data format, we can check if isinstance tree,float - which wouldn't work for multiple reasons in this assignment\n",
    "# I have opted for the decision tree to be a dictionary format instead of a tuple, both are hard to read though\n",
    "# instead of checking for a float, because I have set a key for when a node becomes a prediciton, we check for the key \"prediction_this_node\" each time we call this function, and when it finds this key it returns\n",
    "# the prediction value that is associated with that node\n",
    "  if \"prediction_this_node\" in individual_tree:\n",
    "    return individual_tree[\"prediction_this_node\"]\n",
    "\n",
    "  # we need to pull the feature and split point values from each dictionary entry to traverse further down, once we have the feature and the split we can test the value of the feature of the sample datapoint against the split point\n",
    "  # so we use the keys to get the values\n",
    "  current_feature = individual_tree[\"feature\"]\n",
    "  current_split_point = individual_tree[\"split_point\"]\n",
    "\n",
    "\n",
    "  # if the value for this samples feature value is less than the split point we traverse to the left child node\n",
    "  # if the value for this samples feature value is greater than or equal to the split point then we traverse to the right child node\n",
    "  # this traversal will come to an end when we encounter the key indicating a prediciton node\n",
    "  if test_data[current_feature] < current_split_point:\n",
    "    return predict_traversal(individual_tree[\"left_leaf_node\"], test_data) # go to the next left node\n",
    "  else:\n",
    "    return predict_traversal(individual_tree[\"right_leaf_node\"], test_data) # go to the next right node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56eb06-3a01-4677-bead-9a22b34b27fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_prediction(forest_dict_trees, test_data):\n",
    "  agg_sum = 0 # default prediction sum\n",
    "\n",
    "  # here we iterate through the tree,oob value pairs in each of the decision trees in our forest dicitonary\n",
    "  # tree index: tree-dict, oob array in the items list of our forest dictionary\n",
    "  # for each entry/item/key:value pair we grab the tree in the value and we traverse it given our test_data sample that we pass into the aggregate_prediction function\n",
    "  # so we call the individual tree prediction function for each each tree and use the same sample data for each tree as we want an aggregate prediction acrros all trees in the forest\n",
    "  for tree_index, (individual_tree, oob_indices) in forest_dict_trees.items():\n",
    "    value = predict_traversal(individual_tree, test_data) # traverse the tree and get a prediction\n",
    "    agg_sum += value # we add the prediction to our sum of predicitons - we could also add each prediciton to a list and then use np.mean to get the avg, but I did a sum then divided by the num trees\n",
    "\n",
    "  agg_prediction = agg_sum / len(forest_dict_trees) # we divide the sum of the predictions by the number of trees we have in our forest\n",
    "  return agg_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f8d8f-744b-4375-ba9d-2349f8b54ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the forest\n",
    "# determining the bootstrap sample size from the data passed into main, whereas num trees and num randomized features are currently being passed into main\n",
    "# its likely better to have them set as variables within the function that calls the forest creator\n",
    "def main3(data, number_of_trees, num_randomized_features):\n",
    "  bootstrap_sample_size = data.shape[0]\n",
    "  forest_dict_trees = random_forest_regressor_from_scratch(data, label_column_name, number_of_trees, num_randomized_features, bootstrap_sample_size)\n",
    "  return forest_dict_trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a8167-36c7-4a67-b405-87fb2d227c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling main to create the forest with num trees, the number of randomized functions\n",
    "mcnair_data = data_sample_train.copy()\n",
    "label_column_name = 'price'\n",
    "num_randomized_features = int(np.sqrt((mcnair_data.shape[1])-1))\n",
    "# 10 trees is quick ~ 2 mins, 100 is taking at least 20 minutes (150 was 32mins) but testing results started getting better\n",
    "# I do not believe offloading this to cuda or metal will beat a cpu (I am not certain you even can), am I wrong about this? If this was a nueral network it would ofcourse be better\n",
    "number_of_trees =  100 # ((mcnair_data.shape[1])-1) ** 2 # 17 ** 2 takes a while\n",
    "\n",
    "forest_dict_trees = main3(mcnair_data, number_of_trees, number_of_random_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76525b98-08e7-41b7-b8ca-c5d5606b8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnair_data.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211b408-73e6-4a01-a112-2144ebd0f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_prediction(forest_dict_trees, mcnair_data.iloc[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1844e39-3cd1-4778-8fb6-f6d9f358a1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c2c0f-6f3e-4bcd-b563-d16ccc8fd3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c45d8-2fbe-4fe2-84f9-29f87d003818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af050aab-48f6-44b6-bc96-464c5b59df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct classifications / total classifications\n",
    "# true positive + true negative / tp + tn + false pos +false neg\n",
    "def accuracy (classifications):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e539c78-b487-43b7-86d1-38c037cd833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many predicted positive are correct - how many of the predicted positives were actually correct\n",
    "# true positive / tp + false positives\n",
    "# false positives are important then use this\n",
    "\n",
    "def precision (classifications):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407b09b-45bd-4817-8b88-1b27cbd68118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity - how many actual positives were found - true positives divided by all positives, false negatives are positives\n",
    "# true positive / tp + false negatives\n",
    "# when false negatives are important use this\n",
    "def recall (classifications):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89906821-1bf0-4c08-a5f3-7bcbe6ddc80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for imbalanced datasets - balance between precision and recall\n",
    "\n",
    "def f1_score (classifications):\n",
    "    precision = precision (classifications)\n",
    "    recall = recall (classifications)\n",
    "    f1 = 2 * ( (precision * recall) / (precision + recall) )\n",
    "    \n",
    "    return precision, recall, f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f40746-ea39-444b-b2c8-93460c381133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3fd89-10d7-40ae-918b-3dd395081b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1d86a-cd25-45c3-aebb-8fb0421baa1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfaeb7a-38d7-4ba1-8df4-3b8e28f966fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
