# cs601-project
cs601 RF Classifier project

Kelton McNair
CS601 RF Classifier Project
Dr. Majid Afshar

I chose to use the Adult Income dataset for my project, this is a binary classification dataset as there are only two labels in the target/label/income feature column of the dataset. I chose to download the dataset and store it locally, but I provided the code from ucimlrepo in which you can access it directly. 

The dataset has roughly 31000 starting samples and 15 columns, there is also a training dataset that you can retrieve but it needs more pre-processing as it's formatting is far from what the main dataset is. I had done some pre-processing with it in my earlier commits but I ultimately decided not to merge it with the main dataset or use it for testing. If I were to expand on the project I would work with processing the data, merging the two datasets and then creating large test and train datasets. 
When it came to pre-processing there were no strictly "missing"/NA/NULL values like you can find in some datasets, from what I can tell this was was input with '?' denoting a missing field in a sample submission. Upon checking how many of the samples has a '?' in our of their values I found that only ~7.5% of the samples had a missing value so I opted to remove them. In my code I left a few questions to myself for figuring out if you can simply treat '?' as its own feature category or not, I need to look into that; and if not, then what is the best methodolgy for value replacement.
For my encoding I chose to implement bianry encoding for my label/feature column and for all other categorical columns I used oneHot/Dummy encoding, creating binary columns for each features unique categories. For each new oneHot category column, if its original value matches the new colum then it received a 1 as its value and 0 if it was not the new column for that category. The issue with this is making the dataset features sparse, I increased the fatures from 15 to 105. I also could have used ordinal coding for at-least the categorical col of educaiton. There was already a numerical education level column and I wanted to see how creating its categorical equivalent into a binary columns would affect the model. I think it would maybe be possible to use another type of encoding on some of the categories but most other options would impose an ordinal scheme to them which may affect the model. I know OneHot and dummy encoding are typically the go to methods for this dataset and for many others where featuresa re not ordinal. 

I implemented my own version of train_test_split to create stratified test and train datasets from my one bulk/starting dataset, I made sure that the same proportion of samples from each label class were split accordingly into the two datasets. In this dataset you have roughly 3x the number of '<=50k' income samples compared to the '>50k', so makign sure the same proportions were set aside for testing and training was important. 

When it came to adapting my assignment 2 random forest regressor model only mininmal changes were required. One big change was with my bootstrap function, instead of passing in the entire training dataset I needed to make sure I was training each decision on a more balanced version of the training dataset. Two options I visited were either to oversample my '>50k' class samples or downsample my '=<50k' samples. I chose to down-sample which has its drawbacks, between decision trees more of the '>50k' samples were seen by every single tree, meaning if there is any overfitting the model it may be towards those samples. In downsampling the '<=50k' label class I made the trianing data more sparse between each decision tree, so it takes a larger number of trees to see all of those samples in the trainind data. I would like to compare how the model performs how it is now versus oversampling the lower quanity label class. 

Instead of using MSE or MAE I implemented both Gini Impurity and Entropy, the regression logic worked with both because no matter which of the two you use, mathmatically the lowest "error" value returned by either will be the correct choice because the highest entropy information gain will come from the lowest weighted leaf node entropy at each split. 

The change to my decision tree build function was as simple as changing the prediciton from the mean of the label values in the prediciton node to taking the mode of the label class values in that node. We don't need to take the mean of the values in a classificaiton model, instead we want to return which class has the majority of samples in that model to prediciton a label binary classificaiton.

I added distinct parameters to my random forest classifier function for the criterion-equation, max depth, min samples to split a node and the min number of resulting samples in eahc leaf. Before I had declared them as variables inside of the classifier function but now I pass them through that function as parameters to my decision tree builder function. 

The last major change was to my aggreagate prediciton function where now instead of taking label value predicitons and taking the mean of them from all the decission trees, creating a random forest prediciton, we instead gatheer all of the label binary classification votes from each individual decision tree in that function and then return the majority vote as the random forest classifier label classification vote. So instead of taking the mean of the decision tree predicitons we take the mode of them, when stored in a list/series.

I then built a confusion matrix function that takes every sample in the test dataset, uses the random forest aggregate prediction function to make a binary classification label prediction and use the prediction from each sample versus its true label class to build my confusion matrix. We incrememted the True_positive, True_Negative, False_Positive and False_Negative counters for each prediciton that fell into their respective buckets and we were able to create a confusion matrix.


For the result of my from_scratch random forest classifer against the two different sklearn classifers that I implemented I got beat by both other models in in accuracy, precsion and F1-Score, but I was able to beat both with in recall score. This happened for all the different number of trees that I built all 3 models with, I tried to keep the number of trees similar. I am not entirely sure why I lost in every area except for recall, every time. I wonder if it has to do with my splitting mask, I use >= and < I wonder if sklearn uses the oppsite. The two sklearn models performed very similarly and are more fined tuned than my model. I think with different and possibly more hyper parameters I could get closer to their results. 

I think that with more fine-tuning I could have increased all the performance of all 3 models and gotten mine closer to the sklearn model's accuracy, precision and f1 scores. 


I was not able to implement the multiprocessing that you will see in my commits and at the end of my notebook as I broke my laptop while I was traveling which cost me some time. 
